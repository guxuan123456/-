#	数学之美读书笔记
	作者：顾轩
	日期：2020.1.19
[toc]
##	第1章	文字和语言 VS 数字和信息

- 文字和语言与数学，从产生起原本就具有相通性，虽然它们的发展一度分道扬镳，但是最终还是能走到一起。
- 数字、文字和自然语言一样，都是信息的载体。

###	1、信息
- 开始时我们的祖先采用声音来传播信息。比如可能会用特定的声音，如“呀呀呀”表示“那里有一只熊”。
- 早期信息很少，不需要语言和数字，但是随着人类进步和文明的发展，所表达信息越来越多，不再是几种声音可以表达，因此语言产生。
- 原始人通信方式和今天的通信模型没有什么不同。
  ![](1.jpg)

###	2、文字和数字
- 开始利用声音传递信息，，后来信息变多，产生了语言，，当语言复杂和繁多到一定程度，人类无法记住所有词汇，便需要文字进行高效记录，由此文字产生。
- 作者举了一些文字记录历史的例子，比如罗塞塔石碑的破译，对从事自然语言处理的学者来说有如下两个意义：
	-	信息的的冗余是信息安全的保障，对信道编码有指导意义，信道编码就是提高信息的冗余度，从而提高信息传输的可靠性，不容易出错。
	-	语言的数据，我们称之为语料，尤其是双语或者多语的对照语料对翻译至关重要，他是我们从事机器翻译研究的基础。采用计算机和数学工具，可以在短时间内实现目的。
	
- 数字出现在人们的财产多到需要数一数才能搞清楚有多少的时候，早期数字并没有书写的形式，而是掰指头，这便是现在采用十进制的原因。
- 位进制的发明（我们常说的逢十进一）表明祖先开始懂得对数量进行编码。
- 玛雅文明采用的是二十进制，算上了脚趾头。
- 数字中的解码：在中国，采用规则为乘法；而在罗马，采用的规则是加减法，小数字出现在大数字左边为减，右边为加。
- 阿拉伯数字是描述数字更加有效的方式，但其实它们是由印度人发明的。标志着数字和文字的分离。
###	3、文字和语言背后的数学
- 信息的编码：象形文字到拼音文字是一个飞跃，由物体的外表到抽象，采用了对信息的编码且编码合理，常用字笔画少，生僻字笔画多。
- 信息压缩和解压缩：古代文言文非常简洁难懂但相同时期口语却和现在相差不大。与现在信息的传送很相似，信息在传递前需尽可能地压缩，然后再接受端解压缩。两个人讲话说的快是一个宽信道，无需压缩；书写来得慢是一个窄信道，需要压缩和解压缩。
- 信息校验：《圣经》的创作经过几个世纪，为了避免出错，在每一行进行一个校验码，这与我们现代通信系统中差错分析中的校验极为相似。
- 语法：语法是语言的编码和解码规则，任何语言都有语法覆盖不到的地方。设计到一个语言学研究方向的问题：语言对还是语法对。语言坚持从真实的语句文本出发，后者坚持从规则出发。最终证明，自然语言处理采用从语料出发，即基于统计规则。

##	第2章	自然语言处理——从统计到规则
人类对机器理解自然语言的认识走了一条大弯路。早期的研究中采用了基于规则的方法，虽然解决了简单的问题，但是无法从根本上将自然语言理解实用化。20多年后，采用基于统计规则的方法进行自然语言处理，才有突破性的进展和实用的产品。
###	1、机器智能
最早提出机器智能设想是计算机科学家图灵——图灵测试。
达特茅斯夏季人工智能研究会议——（讨论当时未解决的问题：人工智能、自然语言处理、神经网络）

实现人工智能的第一步让计算机理解自然语言，这样才能让机器完成翻译或者语音识别等只有人类才能完成的事情，当时想法是让机器拥有向人类的智能，但这几乎是不可能的。如何让计算机理解自然语言：
<center>分析语法和获取语义</center>
早期的思路是分析语法和获取语义，但是语言这个东西，一句话可能有各种理解和各种表述，所以不是一个很好的方法，所取得效果也不是很好，后来就变成基于统计规则的自然语言处理了。

当时语义分析和句法分析采用的是分析树的形式，以“徐志摩喜欢林徽因”为例：

下面是早期对自然语言处理的理解：

  ![](2.png)

例句的语法分析树：


<img src="语法分析树.png" width = 100% height = 100% />

###	2、从规则到统计
1970年以后统计语言学的出现是自然语言处理获得新生，关键人物弗里德里克.贾利尼克和他领导的IBM华生实验室。采用基于统计的方法，IBM将语音识别率从70%提高到90%，规模从几百单词上升到几万单词。

现在自然语言处理从原来单纯的句法分析和语义理解变成了非常贴近生活的机器翻译、语音识别、文本到数据库自动生成、数据挖掘和知识的获取。

基于统计的自然语言处理，在数学模型上和通信是相通的，甚至是相同的，数学意义上自然语言处理和语言的初衷——通信联系到一起了。但是，科学家们用了几十年才认识到这个联系.


##	第3章	统计语言模型
统计语言模型是自然语言处理的基础，并且被广泛应用于机器翻译、语音识别、印刷体或手写体识别、拼音纠错、汉字输入和文献查询。
### 用数学方法描述语言规律

一个句子是否合理主要看其可能性大小如何。
假设S是一个有意义的句子，现在来看她的概率：
$$ P(S) = P(w_1,w_2,w_3,...,w_n)$$

 $P(w_1,w_2,w_3,...,w_n)$可转化为：
 $$  P(w_1,w_2,w_3,...,w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1,w_2) \cdot\cdot\cdot P(w_n|w_1,w_2,...,w_{n-1})$$

根据马尔可夫假设：任意一个词$w_i$出现的概率只与$w_{i-1}$有关。
 $$  P(S) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdot\cdot\cdot P(w_n|w_{n-1})$$

 上式代表二元模型，一个词的概率由其前面$N-1$个词决定，则成为$N$元模型。

现在只需要求$ P(w_i|w_{i-1})$,由概率论知识可得：
$$  P(w_i|w_{i-1}) = \frac{P(w_{i-1},w_i)}{P(w_{i-1})}$$

右边的概率很好求。假设知道语料有多大，只需要在语料中寻找 #$(w_{i-1},w_i)$ 和# $(w_i)$出现的次数，得到相对频度后：
$$   P(w_i|w_{i-1})\approx \frac{\#(w_{i-1},w_i)}{\#(w_{i-1})}$$

 如果上式中的$(w_{i-1},w_i)$在语料库中没有出现或者只出现一两次，估算概率很难，其相关解决办法再延伸阅读中介绍。
### 延伸阅读：统计语言模型的工程诀窍
#### 高阶语言模型
前面假设某处的概率只和之前一词有关，有时这过于简单

假定与前面$N-1$个词都有关系,那么$w_i$的概率只取决于之前$N-1$个词。这样的假设称为$N-1$阶马尔科夫假设。对应语言模型为$N元模型$。实际中较常见的是$N = 3$的三元模型。


#### 模型的训练：零概率问题和平滑方法

上述式子成立的前提是大数定律，因此为了减少误差，需要增加数据量，但即使数据量很大也可能出现零概率或者统计量不足的情况。可以想象在很大的数据量下基本上大部分的概率为0.这种模型称为不平滑，所以需要进行平滑处理。

古德——图灵设计的原理：没看见的事件不可认为概率为0，需从概率的总量分配一个很小的比例给没有看到的事件。因此需要将所有看得见的事件概率调小一点(统计中相信可靠的数据，对不可靠的数据应该打折扣)。

假设：在语料库中出现$r$次的词有$N_r$个，特别的，未出现的词数量为$N_0$，语料库的大小为$N$.则:
$$ N = \sum ^ \infty _{r=1} rN_r$$

出现r次的词在整个语料库中的相对频度为$rN_r/N$，不优化则用此值处理，优化可用一个比r小一点的值$d_r$进行处理。古德——图灵估计如下：
$$ d_r = (r+1) \cdot N_{r+1} /N_r$$

此时相当于将出现的词的频率降低，分配给没出现的词。在实际中，出现次数高于一定阈值的词不进行频率下调处理，只对频率较低的词进行处理。这样便完成了零概率处理和平滑处理。

同样道理，二元模型也可以进行相似的处理，即出现次数高于一定阈值的词不进行频率下调处理，将频率给未出现的词。二元模型概率公式如下：
$$P(w_i|w_{i-1})= \begin{cases}
f(w_i|w_{i-1}) & if \#(w_{i-1},w_i) \geq T\\
f_{gt}(w_i|w_{i-1}) & if  0< \#(w_{i-1},w_i) < T \\
Q(w_{i-1})\cdot f(w_i) & otherwise
\end{cases}$$

其中T是阈值，一般为8~10，$f_{gt}()$为经过古德——图灵估计后的相对频度。Q如下：
$$ Q(w_{i-1}) = \frac{1-\displaystyle\sum_{w_i seen}P(w_i|w_{i-1})}{\displaystyle\sum_{w_i seen}f(w_i)}$$

上式为卡茨退避法，同理三元模型和N元模型也有相应的表达式。一般只取三元模型，所以这里只列举三元模型概率公式：

$$P(w_i|w_{i-2},w_{i-1})= \begin{cases}
f(w_i|w_{i-2},w_{i-1}) & if \#(w_{i-2},w_{i-1},w_i) \geq T\\
f_{gt}(w_i|w_{i-2},w_{i-1}) & if  0< \#(w_{i-2},w_{i-1},w_i) < T \\
Q(w_{i-2},w_{i-1})\cdot f(w_i,w_{i-1}) & otherwise
\end{cases}$$

书中还介绍了一个线性插值法，但结果比卡茨退避法较差：
$$ P(w_i|w_{i-1}) = \lambda(w_{i-2},w_{i-1})\cdot f(w_i|w_{i-2},w_{i-1}) + \lambda (w_{i-1})\cdot f(w_i|w_{i-1}) + \lambda f(w_i)$$
#### 语料的选取问题
语料的选取也相当重要，如某个网页应用是网页搜索，则该模型的训练数据应该是杂乱的网页数据和用户输入的搜索串。

训练数据一般来说越多越好，大数据时概率模型的参数可以估计的很准确。

再训练数据和应用数据一致并且训练量足够大的情况下，训练语料的噪音高低对效果也会产生影响。训练之前应该对训练数据进行预处理，一般称为去噪。

##	第4章	谈谈分词
中文分词是中文信息处理的基础，它同样走过一段弯路，目前依靠统计语言模型已经基本解决了问题。

### 中文分词方法的演变
利用统计语言模型进行自然语言处理是建立在词的基础之上，西方拼音语言词之间有明显分界符，但中文等词之间无明显分界符，先对句子进行分词才能进行语言处理。

最简单的处理方法是查字典，将句子从左到右扫描，遇到字典中的词就标识出来，遇到复合词就找·最长的词进行匹配。后来该方法发展为最少词数的分词理论，但是该方法对有二义性的词进行分割时便无能为力，如：

		发展中国家：正确分割是：发展-中-国家，但是查字典便会分成：发展-中国-家
		上海大学城书店：正确分割:上海-大学城-书店，查字典便会分成：上海大学-城-书店。

二义性一直是一个较难解决的问题，后来该问题由郭进博士利用统计语言模型解决了二义性问题，具体方法如下：

假设一个句子$S$有几种分词方法，假设有以下三种：
$$A_1,A_2,A_3,...,A_k$$$$B_1,B_2,B_3,...,B_m$$$$C_1,C_2,C_3,...,C_n$$

其中A、B、C都是汉语的词，则最好的一种分词方法应该保证分完词后该句子出现的概率最大，即：若$A_1,A_2,A_3,...,A_k$是最好的效果，有概率满足：

$$P(A_1,A_2,A_3,...,A_k) > P(B_1,B_2,B_3,...,B_m)$$ $$P(A_1,A_2,A_3,...,A_k) > P(C_1,C_2,C_3,...,C_n)$$

所以利用上一章的统计语言模型计算每种分词后句子出现的概率，并找出其中概率最大便能找到分词结果。

但是如果采用穷举法，计算量较大。可以看成是一个动态规划问题，利用维特比算法快速找到最佳分词，模型如下：


![分词器示意图](绘图1.png)


应用不同，汉语分词的颗粒度大小不同，在机器翻译中，颗粒度应该大一些，应将“北京大学”当作一个整体，而在语音识别中应该分为"北京"和“大学”两个词。不同的应用应该有不同的分词系统。

分词技术并不只是针对亚洲语言，也可以应用到英语中，主要是手写体识别时单词之间的空格不清晰，中文分词方法可以帮助判别单词的边界。


### 延伸阅读：如何衡量分词的结果

#### 分词的一致性
衡量分词结果的好与坏，需要对计算机结果与人工分词的结果进行比较。但是不同的人对同一个句子可能有不同的分词方法，而且几乎都有道理。

统计语言模型用于分词之前，分词的准确率较低，可提升空间较大，人切分的差异影响较小，可以根据分词结果与人工切分的比较进行衡量。

但是当应用统计语言模型时，分词准确率提高，不同分词器产生的结果的差异要远远小于不同人之间看法的差异。这时简单依靠与人工分词的结果比较来衡量分词器的准确性就很难。但是幸运的是一般现在采用统计语言模型效果都差不到哪里去。

#### 词的颗粒度和层次
人工分词产生不一致的原因主要在于人们对词的颗粒度认可问题。

不同应用中会有一种颗粒度比另一种更好的情况。机器翻译中颗粒度大的效果好。网页搜索中相对而言颗粒度小的效果好。

较好的做法是让一个分词器支持不同层次的词的切分。有不同的应用自行决定切分的颗粒度。原理和实现介绍如下：
- 需要一个基本词表和一个复合词表。基本词表包括像“清华”这样不可再分的词。复合词表包含复合词以及他们有哪些基本词构成。
- 需要根据基本词表和复合词表各建立一个语言模型，如$L1$和$L2$.
- 根据基本词表和语言模型L1对句子进行分词，得到小颗粒的分词结果，基本词比较稳定，小颗粒度的分词基本不需要额外的研究。
- 在上述基础上利用复合词表和语言模型L2进行第二次分词。输入基本词串，输出复合词串，数据库改变了但是分词器本身前后完全相同。
  
分词性的准确性问题，不一致性可以分为错误和颗粒度不一致两种。错误包含越界型错误和覆盖性错误。人工分词的不一致性多属于颗粒度不一致。

对于某些应用须尽可能找到各种复合词，需要继续进行数据挖掘，不断完善复合词的词典，也是近几年中文分词工作的重点。

##	第5章	隐马尔可夫模型
隐马尔科夫模型最初应用于通信领域，进而推广到语音和语言处理中，成为连接自然语言处理的通信的桥梁。同时隐马尔科夫模型也是机器学习主要工具之一。
### 	通信模型
通信的本质是一个编解码和传输的过程。早期自然语言处理主要集中在语法语义和知识表述上，与通信相差甚远。后来将自然语言处理的问题回归到通信系统中的解码问题（即由收到的信号推测别人发送的信号），相关难题迎刃而解。

一个典型的通信系统：发送者、编码、信道、解码、接收者。
<img src="通信模型.png" width = 100% height = 100% />

这里我们关注的是如何根据接收到的信息来推测出发送的信号。通信模型和自然语言处理模型有相通之处。语音识别是根据收到的语音猜测说话者表达的意思，机器翻译和自动纠错也可以近似理解。都是根据接收信息来推测出发送的信息。由此几乎所有自然语言处理问题都可以等加成通信中的解码问题。

现在要求出所有信息源中最可能产生观测信号的那一组信息。即求出使条件概率$P(S_1,S_2,S_3,...|O_1,O_2,O_3,...)$达到最大值的那串信息：$S_1,S_2,S_3,...$即：

$$
 S_1,S_2,S_3,... = \underset{all \, S_1,S_2,S_3,\ldots}{\operatorname{Arg\,Max}}\, P(S_1,S_2,S_3,...|O_1,O_2,O_3,...).   
$$

上述概不好算，转化为如下公式：
$$ \frac{P(O_1,O_2,O_3,...|S_1,S_2,S_3,...)\cdot P(S_1,S_2,S_3,...)}{P(O_1,O_2,O_3,...)}$$

在上式中，一旦接受端接收信号。则$P(O_1,O_2,O_3,...)$概率成为一个定值，可以忽略常数。由此可以得到上式公式等价为：
$$P(O_1,O_2,O_3,...|S_1,S_2,S_3,...)\cdot P(S_1,S_2,S_3,...) $$

这个公式便可以根据隐马尔可夫模型来估计。上式个部分意义较为明显，不再赘述。

### 隐马尔科夫模型

随机过程和随机变量的区别(举个例子)：
随机变量就是一个事件有不同的可能性，随机过程表示任何一个状态t，对应的$S_t$都是随机的。随机过程相当于在随机变量上加一个时间维度，不同时间产生结果也不同。

$S_1,S_2,S_3,...S_t...$看成北京每天的最高气温，这里面每一个状态$S_t$都是随机的。每一天的最高气温与这段时间以前的最高气温是相关的，随机过程有两个维度的不确定性。

马尔科夫假设类似于某一天的最高气温只与前一天的最高气温有关，即：$P(s_t|s_1,s_2,s_3,...,s_{t-1}) = P(s_t|s_{t-1})$.符合此假设的随机过程称为马尔可夫过程，也称马尔科夫链。

隐含马尔可夫模型是马尔可夫链的一个扩展：任一时刻的状态$S_t$是不可见的。无法通过观察$S_1.S_2,S_3,...S_T$来推测转移概率等参数。但每个时刻隐含马尔可夫模型会输出一个符号$O_t$，且$O_t$仅和$S_t$相关，称为独立输出假设。其相应的模型如下：
<img src="隐马尔可夫.png" width = % height = 50% />

可以计算某个特定序列$S_1,S_2,S_3,...S_t...$产生输出符号$P(O_1,O_2,O_3,...)$的概率：

$$P(S_1,S_2,S_3,...O_1,O_2,O_3,...) = \prod_{t} P(S_t|S_{t-1})\cdot P(O_t|S_t)$$

前面通信模型中所得的最后一个式子便是上式。由此可以利用隐马尔科夫模型解决自然语言处理问题。再利用维特比算法便可以找出要识别的句子$S_1,S_2,S_3,...S_t...$

#### 延伸阅读：隐含马尔可夫模型的训练
隐含马尔可夫有三个问题：
1. 给定一个模型，计算某个特定输出序列的概率。
2. 给定一个模型和特定输出序列，找出最可能产生这个输出的状态序列
3. 给定足够的观测数据，如何估计隐马尔科夫模型的参数

第一个问题有对应算法。第二个问题有维特比算法。现在讨论模型的训练问题：

利用隐马尔可夫模型解决实际问题，需要知道转移概率，即：$P(S_t|S_{T-1})$。需要知道每个状态$S_t$产生相应输出$O_t$的概率$P(O_t|S_t)$,也成为生成概率。这些概率便是隐马尔科夫模型去的参数，计算模型称为模型的训练。

由条件概率可知：
$$P(O_t|S_t) = \frac{P(O_t,S_t)}{S_t}$$
$$P(S_t|S_{t-1}) = \frac{P(S_{t-1},S_t)}{S_{t-1}}$$

对于上式，如果有足够多的人工标记,比如知道经过状态$S_t$有多少次数$\#(S_t)$已经从状态
$S_t$进入状态$O_t$的数量$\#(O_t)$.便可以得到生成概率：

$$P(O_t|S_t) = \frac{\#(O_t,S_t)}{\#(S_t)}$$
同理得出转移概率:
$$P(S_t|S_{t-1}) = \frac{\#(W_{t-1},W_t)}{\#(W_{t-1})}$$

上述得到参数的方式(有监督的训练)需要大量人工标注的数字，实际上很多应用无法做到。因此训练隐马尔科夫模型更实用方式通过输出序列$P(O_1,O_2,O_3,...)$推算出模型参数，这类方法称为无监督的训练方法，主要使用鲍姆-韦尔奇算法。

通过输出信号得出隐马尔科夫模型可能会有多个，但是总有一个模型$M_{\theta 2}$比$M_{\theta 1}$更有可能产生观测到的输出。鲍姆-韦尔奇算法就是用来寻找这个最可能的模型$M_\theta$

主要思想如下：

- 首先找到能够产生输出序列$O$的参数模型（明显一定存在，转移概率和输出概率均为均匀分布时模型可以产生任何输出）
- 有了初始模型$M_\theta 0$，在此基础上找一个更好的模型，在解决第一个第二个问题得情况下，可以算出此模型产生$O$的概率$P(O|M_{\theta 0})$,并且可以得到模型产生$O$
的路径等等，此时相当于标注的训练数据，再根据前面说的公式计算出一个新的模型$M_\theta 1$，由此完成一次迭代。
- 由此不断出发，可以不断迭代找到更好的模型，知道模型质量不在有明显提高为止。

该算法不断估计模型参数以实现输出概率(目标函数)最大化，这个过程称为期望值最大化，简称EM过程。但这个过程一般都是局部最优点而不是全局最优点(目标函数是凸函数，如信息熵是可得全局最优点)。所以无监督训练模型一般效果比有监督模型差一点。



##	第6章	信息的度量和作用
信息是可以量化度量的。信息熵不仅是对信息的量化度量，也是信息论的基础。对于通信、数据压缩、自然语言处理有很强的指导意义。

### 信息熵

### 信息的作用

### 互信息

### 延伸阅读：相对熵


##	第7章	贾里尼克和现代语言处理
作为现代自然语言处理的奠基者，贾里尼克教授成功的将数学原理应用于自然语言处理领域，其一生富有传奇色彩。


关于贾里尼克教授的生平百度上十分详细。作者主要以此文纪念贾里尼克博士。

##	第8章	简单之美——布尔代数和搜索引擎
布尔代数虽然十分简单，确实计算机科学的基础，它不仅把逻辑和数学合二为一，而且给了我们一个全新的角度看待世界，**开创了数字化时代**。

###	1、布尔代数

由于人类有十根手指，所以一般采用的都是十进制，但是往往二进制是最简单的，因为其只有0和1两个数字，当然二进制也是数字化的基础。

二进制发展历史（可以用来装逼，实际用处其实不大）：

- 中国古代的阴阳学说可认为是二进制的最早雏形
- 公元前2-5世纪时印度学者将二进制作为一个技术系统，但是当时没有使用0和1计数。
- 莱布尼兹完善二进制并使用0和1技术
- 布尔是19世纪英国一位中学教师，提出了布尔运算，运用数学的方法解决逻辑问题，成为数字电路的基础，可以实现逻辑或，逻辑与、取反运算。进而推导出加减乘除、乘方开方等等。借此人们搭建了第一台电子计算机。

二进制不仅是一种计数方式，同时他也能表示逻辑的是与非，由此其可以作为判断，而搜索引擎重点之处在于判断一个网页与关键词的相关性，若能搜索到，则采用是标记，否则为否，由此可见，每个搜索引擎都无法跳出布尔运算的框架。

布尔代数对于数学的意义等同于量子力学对于物理的意义，他们将我们对世界的认识从连续状态扩展到离散状态，布尔世界里，万物都是可以量化的。

###	2、索引

上网搜索资料，基本能在短时间内获取搜索结果，这与建立索引有关。搜索引擎可以看作是图书馆的索引卡片，通过索引找到位置，进而获取结果。

计算机时代索引主要是基于数据库的，数据库的查询语句(SQL)支持各种复杂的逻辑组合。但是背后的原理是布尔运算（这里可以考虑学一手数据库）

搜索引擎会将用户输入转化为一个个关键词之间的布尔运算。然后对关键字进行求与 or 或来得到结果。

最简单的索引结构是一个很长的二进制表，一个关键字在各种文献中是否存在用0和1表示。每一个数对应一个文献。这样便得到一个很长的二进制数，其表头可以是关键字。比如：

	“原子能” ：0100100011000001...
	“应用”   ：0010100110000001...
这样在搜索原子能的应用是就会将上面两个索引表想与，得到结果
	
	“原子能的应用” ：000010000000001...
表示第五篇和第十六篇满足要求，返回给用户结果。

常见的搜索引擎一般将所有的词进行索引，但是此工程很有挑战性(想想多少个词以及多少文献就知道多大了。)而且为了网页排名方便，索引中还要加入其他信息。索引达到一台服务器存储不下，需要通过分布式的方式存储到不同服务器上。

根据网页序号将索引分成很多份。分别存储到不同的服务器上。接受一个查询时，这些查询分发到不同服务器上，所有服务器进行并行处理，将结果返回用户。

##第9章	图论和网络爬虫
##	第10章	PageRank——Google的民主表决式排名技术
##	第11章	如何确定网页和查询的相关性
##	第12章	有限状态机和动态规划——地图与本地搜索的核心技术
##	第13章	Google	AK-47的设计者——阿米特.辛格博士

这一章吴军博士主要是分享一些故事，介绍介绍辛格博士顺便装个逼。

一个好的算法就应该像AK-47那样：简单、有效、可靠性强、容易读懂。

坚持选择简单方案的一个优点在于容易解释每一个步骤和方法背后的道理，不仅便于出了问题时查错，而且容易找到今后改变的目标。（因此对方案的完全理解十分重要）。辛格博士要求对搜索质量的改进方法都能说清理由，这和微软、雅虎把搜索质量的提升当作一个黑盒子完全不同，因此谷歌能保证搜索质量长期稳步提高。后面也提倡通过机器学习改进搜索质量，但要对参数和公式给予合理的解释。

年轻人不要害怕失败，大胆尝试。学习辛格博士坚持每天分析一些搜索结果不好的例子，提醒我们要尽量经常看看自己做得不好的地方。

##	第14章	余弦定理和新闻的分类


2002年，谷歌推出新闻服务，新闻由计算机整理、分类并自动生成的。关键技术在于新闻的自动分类，其用到的技术时数学中常见的余弦定理。

###新闻的特征向量

计算机的本质是做快速计算，因此需要考虑将新闻变成一组可计算的数字，然后设计算法算出任意两篇新闻的相似性。

新闻的信息和词的语义是联系在一起的，同一类新闻的关键用词都是相似的，不同新闻一般不同。在词语中不同词表达的语义重要程度不同，而且一般情况下：

- 含义丰富的实词比“的，地”这样的助词重要，也比“之乎者也”这样的虚词重要

接下来考虑如何对每个实词的重要性进行度量，在网页和查询相关性一章中可以了解到一篇文章中重要的词TF-IDF值就高。于是有与新闻主题有关的实词频率高时IF-IDF值就大，由此可得出一组描述新闻主题的数字：

对新闻中的所有实词，计算出他们的IF-IDF值。将这些值按照对应实词在词汇表中的位置依次排序，便可得到一个向量，如下所示：


 | 单词编号 | 汉字词 |IF-IDF值|
 | ------- | ------ | ------ |
 | 1 | 啊 | 0 |
 | 2 | 啊 |0.0034 |
 |...|...|...|
 |64000|做作|0.075|



如果是64000个数，便得到一个64000维的向量,便可以用这个向量代表这篇新闻，称为新闻的特征向量。向量中每一个维度的大小代表每个词对这篇新闻主题
的贡献。由此一篇新闻变成了一个个数字向量，接着便可以算一算新闻之间的相似性。

###向量距离的度量

上面实现了将新闻转化为数字，同一类新闻一定是某些主题词用得较多，另外一些词用的较少。由此看来，若两篇新闻属于同一类，他们的特征向量在某几个维度的值都比较大，其他维度的值较小。两篇新闻主题是否接近，取决于特征向量是否长得像，因此需要定量衡量特征向量的相似性。

向量实际是多维空间中从原点出发的有向线段（二维坐标系中点可用二维向量描述）。可以通过计算两个向量的夹角来判断新闻地接近程度，由此向量的夹角可以考虑三角形地余弦定理，利用余弦定理求取夹角：

$$ cosA = \frac{b^2+c^2-a^2}{2bc}$$
如果将三角形地两边$b$和$c$看作是两个以$A$为起点的向量，则上述公式等价于：
$$cosA = \frac{<b,c>}{|b|\cdot|c|}$$
其中，分母表示向量$b$和$c$的长度，分子表示两个向量的内积，举个例子：
假设新闻X和新闻Y对应向量如下：
$$x_1,x_2,...,x_{6400}和y_1,y_2,...,y_{6400}$$
则可以得到他们夹角的余弦：
$$ cos\theta = \frac{x_1y_1+x_2y_2+...+x_{6400}y_{6400}}{{\sqrt{x_1^2+x_2^2+...x_{6400}^2}}\cdot{\sqrt{y_1^2+y_2^2+...y_{6400}^2}}}$$
夹角的余弦越接近1，两则新闻越相似，从而可以归为一类；夹角地余弦越小，夹角愈大，两条新闻越不相关。

现在可以将新闻变成数字（特征向量），也有了计算相似性的公式。可以讨论新闻分类的算法，两种情况：
1. 各类新闻特征向量$x_1,x_2,...x_k$已知
2. 新闻类别特征向量未知

对于情况一：
相对简单，求出要分类新闻$Y$的特征向量，求出他和各类新闻特征向量的余弦相似值，由此可以进行分类。

情况二：
较为复杂，采用自底向上不断合并的方法，大致思路如下：
1. 计算所有新闻之间两两余弦相关性，把相似性大于一个阈值的新闻合并为一个小类。这样$N$篇新闻可以合并成$N_1$个小类，$N_1<N$
2. 每个小类中的新闻作为一个整体，计算小类的特征向量，计算小类之间的相似性，然后合并成大一点的小类，结果$N_2<N_1$

这样不断做下去，直到每个类越来越大。某一类太大时其新闻之间相似性就很小，便可以停止迭代过程了。

当时做这个分类的东西是因为一个人不想分论文，便通过这个东西可以将论文分出来。
### 计算余弦的技巧
大数据时利用上述方法的计算量是很大的，尤其是考虑到不断迭代，其计算量
无法接受，在大数据是考虑降低余弦计算的复杂度（可优化部分如下）：
1. 分母部分（向量的长度）不需要重复计算，可将各向量的长度存起来，计算量可以节省2/3
2. 分子上两个向量内积时，只需考虑非零元素，计算的复杂度取决于两向量中的最小值。而向量中的非零元素是很少的，可以根本上降低复杂度。
3. 删除虚词，不仅对提高计算速度有好处，对新闻分类的准确性也有好处，这里虚词实际上相当于噪音，与通信时过滤掉低频噪音是相同道理。

位置得加权也同样重要，出现在文本中不同位置的词的重要性不同，这和老师强调开头结尾、标题、每段第一句是类似的，因此实际中因将相应位置的相对提高一些，对标题和重要位置的词进行额外加权，可提高分类准确性。

本章介绍新闻归类的方法，准确性好，适用于分类文本集合在百万数量级，达到亿级时，计算时间太长。对于大规模文本处理，需要采用下一章的方法。
##	第15章	矩阵运算和文本处理中的两个分类问题
##	第16章	信息指纹极其应用
##	第17章	由电视剧《暗算》所想到的——谈谈密码学的数学原理
##	第18章	闪光的不一定是金子——谈谈搜索引擎反作弊问题和搜索结果的权威性问题
##	第19章	谈谈数学模型的重要性
##	第20章	不要把鸡蛋放到一个篮子里——谈谈最大熵问题
##	第21章	拼音输入法的数学模型
##	第22章	自然语言处理的教父马库斯和他的优秀弟子们
##	第23章	布隆过滤器
##	第24章	马尔科夫链的扩展——贝叶斯网络
##	第25章	条件随机场、文法分析及其他
##	第26章	维特比和他的维特比算法
##	第27章	上帝的算法——期望最大化算法
##	第28章	逻辑回归和搜索广告
##	第29章	各个击破算法和Google云计算的基础
##	第30章	Google大脑和人工神经网络
##	第31章	大数据的威力——谈谈数据的重要性